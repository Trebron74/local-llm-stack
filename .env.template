# Ollama configuration details
OLLAMA_WEBAPI_PORT=11434            # The port on which the Ollama web API will run
OLLAMA_BASE_URL=http://ollama:11434 # Base URL for the Ollama service
OLLAMA_DOCKER_TAG=latest            # Docker tag for the Ollama image (e.g., 'latest')
# LLM models to update
OLLAMA_LLM_MODELS=llama3.1:8b,qwen2.5-coder:1.5b-base,nomic-embed-text:latest

# Open-WebUI configuration details
OPEN_WEBUI_PORT=3000                # The port for the open-webui service
OPEN_WEBUI_DOCKER_TAG=main          # Docker tag for the open-webui image (e.g., 'main' or 'cuda')
OPEN_WEBUI_CORS_ALLOW_ORIGIN=*      # CORS configuration Wildcard for development (not recommended for production)
OPEN_WEBUI_USER_AGENT=Open-WebUI/1.0 (Development) # User-agent for external requests

# GPU settings
NVIDIA_VISIBLE_DEVICES_OLLAMA=0     # Assign GPU 0 to Ollama
NVIDIA_VISIBLE_DEVICES_OPEN_WEBUI=  # Leave empty to disable GPU for open-webui, or set to '0' for shared access

# Shared settings
STACK_TIMEZONE=UTC                  # Global timezone setting
LOG_LEVEL=info                      # Default log level
